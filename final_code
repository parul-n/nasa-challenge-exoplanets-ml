### WHAT I USED FOR GOOGLE COLAB- SKIP IF NOT USING GOOGLE COLAB
from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir("/content/drive/MyDrive/Colab Notebooks")  #put the address where your file lies in Google Drive
!pwd

#-----------------------------------------------------------------------------------------------------------

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import joblib



##Load dataset
df = pd.read_csv("kepler_data.csv", comment="#")
print("Dataset Loaded Successfully")
feature_cols = ["koi_period","koi_duration","koi_depth","koi_ror",
                "koi_teq","koi_insol","koi_steff","koi_srad","koi_model_snr"]
target_col = "koi_pdisposition"
df = df[feature_cols + [target_col]]



##Encode Target
le = LabelEncoder()
df[target_col] = le.fit_transform(df[target_col])
print("\nLabel Mapping:", dict(zip(le.classes_, le.transform(le.classes_))))

X = df[feature_cols]
y = df[target_col]



##Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
print(f"Train shape: {X_train.shape}, Test shape: {X_test.shape}")



##Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)



##Hyperparameter Tuning
#1. Random Forest
rf_param_grid = {
    'n_estimators': [200, 300],
    'max_depth': [8, 12, 15],
    'min_samples_split': [2, 5],
    'class_weight': ['balanced']
}
rf = RandomForestClassifier(random_state=42)
rf_grid = GridSearchCV(rf, rf_param_grid, cv=3, scoring='roc_auc', n_jobs=-1)
rf_grid.fit(X_train_scaled, y_train)
rf_best = rf_grid.best_estimator_
print("\nBest Random Forest Params:", rf_grid.best_params_)



#2. XGBoost
scale_pos_weight = (y_train==0).sum() / (y_train==1).sum()  # handle imbalance
xgb_param_grid = {
    'n_estimators': [200, 300],
    'max_depth': [6, 12],
    'learning_rate': [0.05, 0.1],
    'scale_pos_weight': [1, scale_pos_weight]
}
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb_grid = GridSearchCV(xgb, xgb_param_grid, cv=3, scoring='roc_auc', n_jobs=-1)
xgb_grid.fit(X_train_scaled, y_train)
xgb_best = xgb_grid.best_estimator_
print("Best XGBoost Params:", xgb_grid.best_params_)



##Evaluation Function
def evaluate_model(model, X_test, y_test, name):
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]
    acc = accuracy_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_prob)
    print(f"\n--- {name} ---")
    print(f"Accuracy: {acc:.3f}, ROC-AUC: {roc_auc:.3f}")
    print("\nClassification Report:\n", classification_report(y_test, y_pred))
    print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
    return y_prob



##Evaluate both models
plt.figure(figsize=(7,5))
rf_probs = evaluate_model(rf_best, X_test_scaled, y_test, "Random Forest")
xgb_probs = evaluate_model(xgb_best, X_test_scaled, y_test, "XGBoost")

# Plot ROC curves
fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_probs)
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_probs)
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest AUC={roc_auc_score(y_test, rf_probs):.3f}')
plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost AUC={roc_auc_score(y_test, xgb_probs):.3f}')
plt.plot([0,1],[0,1],'--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC-AUC Comparison')
plt.legend()
plt.grid()
plt.show()



##Save Models and Preprocessing
joblib.dump(rf_best, "rf_model.pkl")
joblib.dump(xgb_best, "xgb_model.pkl")
joblib.dump(scaler, "scaler.pkl")
joblib.dump(le, "label_encoder.pkl")
print("\nModels and preprocessing saved successfully!")

